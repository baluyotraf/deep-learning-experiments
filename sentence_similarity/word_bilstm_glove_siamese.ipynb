{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedded Bidirectional LSTM Siamese Network\n",
    "\n",
    "This notebook shows how to train a sentence similarity/sentence embedding model using word embedding and bidirectional LSTM on a siamese network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import kotoba as kt\n",
    "import narau as nr\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data related constants\n",
    "\n",
    "*   ```DST_```: TFRecord files of the SNLI data\n",
    "*   ```EMBEDDING_FILE```: Path to the Glove embedding file\n",
    "*   ```MODEL_PATH```: Path to save the model checkpoints or training progress. This is formatted with the timestamp of the run\n",
    "*   ```DATA_COUNT```: Number of training data to use. Set to ```None``` to use all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DST_DATA_PATH = os.path.join('data', 'glove.6B')\n",
    "DST_TRAIN_PATH = os.path.join(DST_DATA_PATH, 'train.tfrecord')\n",
    "DST_DEV_PATH = os.path.join(DST_DATA_PATH, 'dev.tfrecord')\n",
    "DST_TEST_PATH = os.path.join(DST_DATA_PATH, 'test.tfrecord')\n",
    "\n",
    "EMBEDDING_FILE = os.path.expanduser('~/Documents/data/glove.6B/glove.6B.100d.txt')\n",
    "\n",
    "MODEL_PATH = os.path.join('word_bilstm_glove_siamese', 'model', '{}')\n",
    "\n",
    "DATA_COUNT = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model related constants\n",
    "\n",
    "*   ```EMBEDDING_WEIGHTS```: Initial weights of the embedding layer\n",
    "*   ```EMBEDDING_SIZE```: Number of tokens in the embedding\n",
    "*   ```EMBEDDING_DIMENSION```: Number of dimensions in the dense representation\n",
    "*   ```EMBEDDING_SPECIAL_TOKENS```: Number of special tokens\n",
    "*   ```EMBEDDING_WITH_PAD```: If embedding includes a padding\n",
    "*   ```EMBEDDING_TRAINABLE```: If embedding is trainable\n",
    "*   ```EMBEDDING_UNITS```: List of units in the embedding transform function\n",
    "*   ```LSTM_UNITS```: List of the LSTM units\n",
    "*   ```LSTM_DROPOUT```: LSTM output dropout probability\n",
    "*   ```PROJECTION_UNITS```: List of units of the projection. Last item determines embedding dimensions\n",
    "*   ```LOSS_MARGIN```: Target distance between different sentences\n",
    "*   ```LEARNING_RATE```: Rate of gradient application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_WEIGHTS = nr.embedding.load_glove_weights(EMBEDDING_FILE)\n",
    "EMBEDDING_SIZE = EMBEDDING_WEIGHTS.shape[0]\n",
    "EMBEDDING_DIMENSION = EMBEDDING_WEIGHTS.shape[1]\n",
    "EMBEDDING_SPECIAL_TOKENS = 2\n",
    "EMBEDDING_WITH_PAD = True\n",
    "EMBEDDING_TRAINABLE = False\n",
    "\n",
    "EMBEDDING_UNITS = [128]\n",
    "\n",
    "LSTM_UNITS = [128]\n",
    "LSTM_DROPOUT = 0.5\n",
    "\n",
    "PROJECTION_UNITS = [1024]\n",
    "\n",
    "LOSS_MARGIN = 1.0\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training data constants\n",
    "\n",
    "*   ```BATCH_SIZE```: Number of data per gradient update\n",
    "*   ```EPOCHS```: Number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the TFRecords to a tensorflow Dataset\n",
    "\n",
    "*   ```parse_example```: Converts TFRecord to a dictionary defined by ```_feature_def```\n",
    "*   ```preprocess_text```: Converts a text to a dense tensor while calculating the length\n",
    "*   ```preprocess_elements```: Processes the text and labels as well as convert them to the model required format\n",
    "*   ```input_fn```: Converts the path of a TFRecord to a dataset. The dataset is also configured as shown below\n",
    "    1.   Load the TFRecords\n",
    "    2.   Parse each TFRecord\n",
    "    3.   Unbatch the THRecord\n",
    "    4.   Preprocess each item\n",
    "    5.   Shuffle is training\n",
    "    6.   Repeat based on epochs\n",
    "    7.   Perform padded bacthing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_feature_def = {\n",
    "    'x1': tf.VarLenFeature(tf.int64),\n",
    "    'x2': tf.VarLenFeature(tf.int64),\n",
    "    'y': tf.FixedLenSequenceFeature([], tf.float32)\n",
    "}\n",
    "\n",
    "def parse_example(example):\n",
    "    context, features = tf.parse_single_sequence_example(example, sequence_features=_feature_def)\n",
    "    return features\n",
    "\n",
    "def preprocess_text(x):\n",
    "    x = tf.sparse_reset_shape(x)\n",
    "    x = tf.sparse_tensor_to_dense(x)\n",
    "    return x, tf.size(x)\n",
    "\n",
    "def preprocess_elements(features):\n",
    "    x1, l1 = preprocess_text(features['x1'])\n",
    "    x2, l2 = preprocess_text(features['x2'])\n",
    "    y = features['y']\n",
    "    inputs = {\n",
    "        'x1': x1,\n",
    "        'len1': l1, \n",
    "        'x2': x2,\n",
    "        'len2': l2\n",
    "    }\n",
    "    return inputs, y\n",
    "\n",
    "def input_fn(filenames, batch_size, epochs, is_training, buffer_multiplier=100):\n",
    "    ds = tf.data.TFRecordDataset(filenames, buffer_size=100, num_parallel_reads=8)\n",
    "    ds = ds.map(parse_example, num_parallel_calls=8)\n",
    "    ds = ds.flat_map(tf.data.Dataset.from_tensor_slices)\n",
    "    if DATA_COUNT:\n",
    "        ds = ds.take(DATA_COUNT)\n",
    "    ds = ds.map(preprocess_elements, num_parallel_calls=8)\n",
    "    if is_training:\n",
    "        ds = ds.shuffle(batch_size * buffer_multiplier)\n",
    "    ds = ds.repeat(epochs)\n",
    "    ds = ds.padded_batch(batch_size, ({'x1': [None], 'l1': [], 'x2': [None], 'l2': []}, []))\n",
    "    ds = ds.prefetch(buffer_multiplier)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tensorflow session configuration\n",
    "This can be set as ```None``` to use the default. The session is part of the tensorflow's low level APIs. Visit https://www.tensorflow.org/guide/graphs to know more about sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_config = tf.ConfigProto()\n",
    "session_config.allow_soft_placement = True\n",
    "session_config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an estimator run configuration\n",
    "Define the run behavior of the estimator using the different parameters\n",
    "*   ```tf_random_seed```: Seed to allow reproducible results\n",
    "*   ```save_summary_steps```: Number of steps before the summary is recorded in tensorboard\n",
    "*   ```save_checkpoints_steps```: Number of steps before a checkpoint is saved\n",
    "*   ```keep_checkpoint_max```: Number of latest checkpoints to keep. ```None``` means do not delete any\n",
    "*   ```session_config```: Configuration of the session used to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.estimator.RunConfig(tf_random_seed=None,\n",
    "                                save_summary_steps=50,\n",
    "                                save_checkpoints_steps=400,\n",
    "                                keep_checkpoint_max=None,\n",
    "                                session_config=session_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model\n",
    "Pass the model constants and other configurations to an instance of a narau estimator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = MODEL_PATH.format(int(time()))\n",
    "\n",
    "clf = nr.estimators.SiameseBiLSTMEmbedding(EMBEDDING_SIZE, \n",
    "                                           EMBEDDING_DIMENSION, \n",
    "                                           EMBEDDING_SPECIAL_TOKENS,\n",
    "                                           EMBEDDING_WITH_PAD, \n",
    "                                           EMBEDDING_WEIGHTS,\n",
    "                                           EMBEDDING_TRAINABLE, \n",
    "                                           EMBEDDING_UNITS,\n",
    "                                           LSTM_UNITS,\n",
    "                                           LSTM_DROPOUT,\n",
    "                                           PROJECTION_UNITS,\n",
    "                                           LOSS_MARGIN,\n",
    "                                           LEARNING_RATE,\n",
    "                                           model_dir,\n",
    "                                           config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining training and evaluation behavior\n",
    "Training and evaluation requires an input function. It is a function that accepts nothing and must return a dataset. Thus the ```input_fn``` created was wrapped with a lambda expression. These are passed to the ```TrainSpec``` and ```EvalSpec``` objects. For the ```EvalSpec``` the steps is set to ```None``` to use the whole data for the evaluation. The other delays/throttles are set low since there is no multi-server configuration that requires these delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = lambda: input_fn([DST_TRAIN_PATH], BATCH_SIZE, EPOCHS, True)\n",
    "dev_input_fn = lambda: input_fn([DST_TEST_PATH], BATCH_SIZE, 1, False)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(train_input_fn)\n",
    "eval_spec = tf.estimator.EvalSpec(dev_input_fn, None, start_delay_secs=0.1, throttle_secs=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating\n",
    "\n",
    "Call the ```train_and_evaluate``` to execute training and evaluation. To view the results of the process run tensorboard. The command ```tensorboard --logdir /path/containing/model_dir --host localhost``` will execute tensorboard on localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.estimator.train_and_evaluate(clf, train_spec, eval_spec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

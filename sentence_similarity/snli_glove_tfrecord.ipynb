{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLI Glove TFRecord\n",
    "\n",
    "This code converts the SNLI dataset into a TFRecord. The dataset is also preprocessed before it is saved as a tfrecord file. The preprocessing done are shown below.\n",
    "\n",
    "Text Preprocessing\n",
    "\n",
    "*   Convert to lower case\n",
    "*   Tokenized with the default NLTK tokenizer\n",
    "*   Converted to IDs using Glove embeddings\n",
    "\n",
    "Label Preprocessing\n",
    "\n",
    "*   Entailment is 1 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all required modules\n",
    "\n",
    "The ```snli``` is the utility python script used for reading the snli dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import snli\n",
    "import tensorflow as tf\n",
    "import kotoba as kt\n",
    "import narau as nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define all path as constants\n",
    "\n",
    "*   ```SRC_*```: SNLI dataset jsonl files\n",
    "*   ```DST_*```: TFRecord save location\n",
    "*   ```EMBEDDING_FILE```: Location of the Glove embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DATA_PATH = os.path.expanduser('~/Documents/data/snli_1.0')\n",
    "SRC_TRAIN_PATH = os.path.join(SRC_DATA_PATH, 'snli_1.0_train.jsonl')\n",
    "SRC_DEV_PATH = os.path.join(SRC_DATA_PATH, 'snli_1.0_dev.jsonl')\n",
    "SRC_TEST_PATH = os.path.join(SRC_DATA_PATH, 'snli_1.0_test.jsonl')\n",
    "\n",
    "DST_DATA_PATH = os.path.join('data', 'glove.6B')\n",
    "DST_TRAIN_PATH = os.path.join(DST_DATA_PATH, 'train.tfrecord')\n",
    "DST_DEV_PATH = os.path.join(DST_DATA_PATH, 'dev.tfrecord')\n",
    "DST_TEST_PATH = os.path.join(DST_DATA_PATH, 'test.tfrecord')\n",
    "\n",
    "EMBEDDING_FILE = os.path.expanduser('~/Documents/data/glove.6B/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a pipeline to process the text\n",
    "\n",
    "Kotoba is used to declare pipelines. The glove file is loaded and then the pipeline is created. The loaded embedding is given to the last stage in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = kt.embedding.Embedding.from_glove_file(EMBEDDING_FILE, ['<PAD>', '<UNK>'], 1)\n",
    "\n",
    "text_pipeline = kt.Pipeline([\n",
    "    kt.LowerCase(),\n",
    "    kt.tokenizer.NLTKTokenizer(),\n",
    "    kt.embedding.EmbedTokenToID(embedding),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a pipeline to process the labels\n",
    "\n",
    "Since there is no predefined kotoba preprocessor for our labels, a function is provided to create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pipeline = kt.Pipeline([\n",
    "    kt.MapItems(lambda x: int(x=='entailment'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and complete the pipeline\n",
    "\n",
    "To make it easier, a pipeline from the file name up to the desired output is created. The text and label pipelines created before are used in this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline = kt.Pipeline([\n",
    "    kt.MapItems(lambda x: snli.read_file(x, label_filters=('contradiction', 'entailment'))),\n",
    "    kt.MapItems(lambda x: (x['sentence1'],\n",
    "                           x['sentence2'],\n",
    "                           x['gold_label'])),\n",
    "    kt.Transpose2D(),\n",
    "    kt.HorizontalPipeline([\n",
    "        text_pipeline,\n",
    "        text_pipeline,\n",
    "        label_pipeline,\n",
    "    ])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline to TFRecord\n",
    "\n",
    "The pipeline will provide the ```x1```, ```x2```, and ```y``` for the network training. To convert this to TFRecords, the ```narau``` helpers are used.\n",
    "\n",
    "TFRecords are just protobufs with a defined format. Below are references that one can use to study them.\n",
    "\n",
    "*   Protocol Buffers: https://developers.google.com/protocol-buffers/\n",
    "*   Sequence Example: https://www.tensorflow.org/api_docs/python/tf/train/SequenceExample\n",
    "*   Tensorflow Examples Code: https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/core/example/example.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tfrecord(src_path, dst_path):\n",
    "    x1, x2, y = data_pipeline.transform(src_path)\n",
    "    ex = nr.example.SequenceExample(\n",
    "        nr.example.FeatureLists({\n",
    "            'x1': nr.example.Int64FeatureList(x1),\n",
    "            'x2': nr.example.Int64FeatureList(x2),\n",
    "            'y': nr.example.FloatFeatureList(y),\n",
    "        })\n",
    "    )\n",
    "    nr.example.save_example(ex, dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the files\n",
    "\n",
    "After all the pipelines and conversion were done, the ```convert_to_tfrecord``` can be easily called to transform the data into the TFRecord format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_tfrecord(SRC_TRAIN_PATH, DST_TRAIN_PATH)\n",
    "convert_to_tfrecord(SRC_TEST_PATH, DST_TEST_PATH)\n",
    "convert_to_tfrecord(SRC_DEV_PATH, DST_DEV_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

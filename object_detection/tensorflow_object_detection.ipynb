{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection using Tensorflow Object Detection API\n",
    "\n",
    "This notebook prepares the dataset for the tensorflow object detection API as well as providing some notes on running and using the files generated by this notebook. This notebook assumes that the format of the data to be converted is similar to the format provided by the RPA team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the modules\n",
    "\n",
    "Make sure that the ```models/research``` is added into your ```PYTHONPATH```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import narau as nr\n",
    "import xmltodict\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from object_detection.protos.string_int_label_map_pb2 import StringIntLabelMap, StringIntLabelMapItem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data related constants\n",
    "*   ```SRC_*```: Paths related to the dataset provided by the RPA team\n",
    "*   ```DST_*```: Save locations of the created files for the object detection API\n",
    "*   ```TRAIN_SPLIT```: The part of the data used for training\n",
    "*   ```CLASSES```: The classes/annotations expected in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PATH = os.path.expanduser('~/Documents/data/rpa')\n",
    "SRC_ANNOTATION_PATH = os.path.join(SRC_PATH, 'annotations')\n",
    "SRC_IMAGES_PATH = os.path.join(SRC_PATH, 'images')\n",
    "\n",
    "DST_LABEL_PATH = 'rpa_label.pbtxt'\n",
    "DST_TRAIN_PATH = 'rpa_train.tfrecord'\n",
    "DST_DEV_PATH = 'rpa_dev.tfrecord'\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "CLASSES = ['textbox']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a protobuf text that defines the labels\n",
    "Create a protobuf text that contains the labels. Each label must be defined using the ```StringIntLabelMapItem``` which are passed ```StringIntLabelMap```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'textbox': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classmap = {cls:idx for idx, cls in enumerate(CLASSES, 1)}\n",
    "labelmap = StringIntLabelMap(item=[StringIntLabelMapItem(id=id_, name=name) \n",
    "                             for name, id_ in classmap.items()])\n",
    "with tf.gfile.GFile(DST_LABEL_PATH, 'w') as f:\n",
    "    f.write(str(labelmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that the dimensions of the images\n",
    "\n",
    "The image dimensions are checked to make sure they still lie within the image and that the boxes dimensions makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dimensions(height, width, xmin, xmax, ymin, ymax):\n",
    "    checks = [\n",
    "        xmin <= width,\n",
    "        xmax <= width,\n",
    "        ymin <= height,\n",
    "        ymax <= height,\n",
    "        xmin <= xmax,\n",
    "        ymin <= ymax, \n",
    "    ]\n",
    "    return all(checks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the image and its annotation to a TFRecord\n",
    "\n",
    "```xmltodict``` is used to parse the annotations while ```narau``` is used as helper in creating the examples for the TFRecord.\n",
    "\n",
    "References:\n",
    "*   Object Detection Documentation on Datasets: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_to_example(annotation_path, image_basedir, classmap):\n",
    "    with open(annotation_path) as f:\n",
    "        annotation = xmltodict.parse(f.read())\n",
    "    annotation = annotation['annotation']\n",
    "    \n",
    "    size = annotation['size']\n",
    "    width = int(size['width'])\n",
    "    height = int(size['height'])\n",
    "    \n",
    "    filename = os.path.basename(annotation['path'])\n",
    "    format_ = os.path.splitext(filename)[-1][1:].lower()\n",
    "    \n",
    "    image_path = os.path.join(image_basedir, filename)\n",
    "    with open(image_path, 'rb') as f:\n",
    "        image_bytes = f.read()\n",
    "        \n",
    "    classes_text = []\n",
    "    xmins = []\n",
    "    ymins = []\n",
    "    xmaxs = []\n",
    "    ymaxs = []\n",
    "    \n",
    "    try:\n",
    "        objects = nr.example._maybe_as_iterable(annotation['object'])\n",
    "    except KeyError:\n",
    "        return None, 0\n",
    "    \n",
    "    count = 0\n",
    "    for obj in objects:\n",
    "        classes_text.append(obj['name'])\n",
    "        \n",
    "        box = obj['bndbox']\n",
    "        xmin = int(box['xmin'])\n",
    "        ymin = int(box['ymin'])\n",
    "        xmax = int(box['xmax'])\n",
    "        ymax = int(box['ymax'])\n",
    "        \n",
    "        if check_dimensions(height, width, xmin, xmax, ymin, ymax):  \n",
    "            xmins.append(xmin/width)\n",
    "            ymins.append(ymin/height)\n",
    "            xmaxs.append(xmax/width)\n",
    "            ymaxs.append(ymax/height)\n",
    "            count += 1\n",
    "        else:\n",
    "            return None, 0\n",
    "    \n",
    "    classes = [classmap[ct] for ct in classes_text]  \n",
    "    example = nr.example.Example(\n",
    "        nr.example.Features({\n",
    "            'image/height': nr.example.Int64Feature(height),\n",
    "            'image/width': nr.example.Int64Feature(width),\n",
    "            'image/filename': nr.example.BytesFeature(filename.encode('utf-8')),\n",
    "            'image/source_id': nr.example.BytesFeature(filename.encode('utf-8')),\n",
    "            'image/encoded': nr.example.BytesFeature(image_bytes),\n",
    "            'image/format': nr.example.BytesFeature(format_.encode('utf-8')),\n",
    "            'image/object/bbox/xmin': nr.example.FloatFeature(xmins),\n",
    "            'image/object/bbox/xmax': nr.example.FloatFeature(xmaxs),\n",
    "            'image/object/bbox/ymin': nr.example.FloatFeature(ymins),\n",
    "            'image/object/bbox/ymax': nr.example.FloatFeature(ymaxs),\n",
    "            'image/object/class/text': nr.example.BytesFeature(map(lambda x: x.encode('utf-8'), classes_text)),\n",
    "            'image/object/class/label': nr.example.Int64Feature(classes), \n",
    "        })\n",
    "    )\n",
    "    return example, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert all images and annotations to examples\n",
    "All of the annotations and images are converted to examples by calling the previously defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotations_to_examples(annotation_basedir, image_basedir, classmap):\n",
    "    examples = []\n",
    "    all_count = 0\n",
    "    for annotation_name in os.listdir(annotation_basedir):\n",
    "        annotation_path = os.path.join(annotation_basedir, annotation_name)\n",
    "        example, count = annotation_to_example(annotation_path, image_basedir, classmap)\n",
    "        if example:\n",
    "            examples.append(example)\n",
    "            all_count += count\n",
    "        else:\n",
    "            print(annotation_path)\n",
    "    print(all_count)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset and save them to TFRecords\n",
    "The dataset is split based on the specified training part and then both are saved to their own TFRecord file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Raffaello Baluyot\\\\Documents\\\\python\\\\notebooks\\\\rpa',\n",
       " 'rpa_train.tfrecord',\n",
       " 'rpa_dev.tfrecord')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = annotations_to_examples(SRC_ANNOTATION_PATH, SRC_IMAGES_PATH, classmap)\n",
    "train_index = int(len(examples) * TRAIN_SPLIT)\n",
    "\n",
    "with tf.python_io.TFRecordWriter(DST_TRAIN_PATH) as writer:\n",
    "    for example in examples[:train_index]:\n",
    "        writer.write(example.SerializeToString())\n",
    "        \n",
    "with tf.python_io.TFRecordWriter(DST_DEV_PATH) as writer:\n",
    "    for example in examples[train_index:]:\n",
    "        writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of datasets is complete\n",
    "Check if the protobuf text and the TFRecord files were created successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Exporting\n",
    "Choose a pre-trained model and then create a pipeline for it. After the pipeline is created, the training can be executed. The trained model can also be exported.\n",
    "\n",
    "References:\n",
    "*   Tensorflow Object Detection Model Zoo: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\n",
    "*   Creating a Pipeline: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/configuring_jobs.md\n",
    "*   Training the Model: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally.md\n",
    "*   Exporting the Trained Model: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/exporting_models.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

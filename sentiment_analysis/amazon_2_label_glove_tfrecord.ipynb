{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon 2 Label Glove TFRecord\n",
    "\n",
    "This code converts the amazon reviews into a TFRecord. The dataset is also preprocessed before it is saved as a tfrecord file. The preprocessing done are shown below.\n",
    "\n",
    "Text Preprocessing\n",
    "\n",
    "*   Convert to lower case\n",
    "*   Tokenized with the default NLTK tokenizer\n",
    "*   Converted to IDs using Glove embeddings\n",
    "\n",
    "Label Preprocessing\n",
    "\n",
    "*   Rating of 5 is 1 (positive) and the 1 is 0 (negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all required modules\n",
    "\n",
    "The ```snli``` is the utility python script used for reading the snli dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import amazon_review\n",
    "import tensorflow as tf\n",
    "import kotoba as kt\n",
    "import narau as nr\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data related information as constants\n",
    "\n",
    "*   ```SRC_*```: Amazon reviews dataset export configuration\n",
    "*   ```DST_*```: TFRecord save location\n",
    "*   ```EMBEDDING_FILE```: Location of the Glove embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DATA_PATH = os.path.expanduser('~/Documents/data/reviews_Books_5.json.gz')\n",
    "SRC_LABELS = (1, 5)\n",
    "SRC_COUNT = 50000\n",
    "SRC_MIN_LENGTH = 1\n",
    "SRC_MAX_LENGTH = 1000\n",
    "SRC_TEST_SPLIT = 0.1\n",
    "SRC_DEV_SPLIT = 0.1\n",
    "\n",
    "DST_DATA_PATH = os.path.join('data', '2_label', 'glove.6B')\n",
    "DST_TRAIN_PATH = os.path.join(DST_DATA_PATH, 'train.tfrecord')\n",
    "DST_DEV_PATH = os.path.join(DST_DATA_PATH, 'dev.tfrecord')\n",
    "DST_TEST_PATH = os.path.join(DST_DATA_PATH, 'test.tfrecord')\n",
    "\n",
    "EMBEDDING_FILE = os.path.expanduser('~/Documents/data/glove.6B/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the dataset\n",
    "Call the ```read``` function from the ```amazon_review``` module to read the amazon review files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_per_label = int(SRC_COUNT/len(SRC_LABELS))\n",
    "filter_length = lambda x: amazon_review.length_between(x, SRC_MIN_LENGTH, SRC_MAX_LENGTH)\n",
    "data = amazon_review.read(SRC_DATA_PATH, SRC_LABELS, count_per_label, filter_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a pipeline to process the text\n",
    "\n",
    "Kotoba is used to declare pipelines. The glove file is loaded and then the pipeline is created. The loaded embedding is given to the last stage in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = kt.embedding.Embedding.from_glove_file(EMBEDDING_FILE, ['<PAD>', '<UNK>'], 1)\n",
    "\n",
    "text_pipeline = kt.Pipeline([\n",
    "    kt.LowerCase(),\n",
    "    kt.tokenizer.NLTKTokenizer(),\n",
    "    kt.embedding.EmbedTokenToID(embedding),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a pipeline to process the labels\n",
    "\n",
    "Since there is no predefined kotoba preprocessor for our labels, a function is provided to create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pipeline = kt.Pipeline([\n",
    "    kt.MapItems(lambda x: int(x==5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and complete the pipeline\n",
    "\n",
    "To make it easier, a pipeline from the file name up to the desired output is created. The text and label pipelines created before are used in this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline = kt.Pipeline([\n",
    "    kt.MapItems(lambda x: (x['reviewText'],\n",
    "                           x['overall'])),\n",
    "    kt.Transpose2D(),\n",
    "    kt.HorizontalPipeline([\n",
    "        text_pipeline,\n",
    "        label_pipeline,\n",
    "    ])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the data with the pipeline\n",
    "Call the ```transform``` method of the ```data_pipeline``` process the data from the amazon review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, labels = data_pipeline.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the dataset to different sets\n",
    "The dataset is divided into training, development, and testing sets. To do this, the ```train_test_split``` function from scikit-learn is used. Since the function can only split a dataset into two, the function is used twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids, train_labels, test_labels = train_test_split(ids, labels, test_size=SRC_TEST_SPLIT, \n",
    "                                                                  random_state=0, stratify=labels)\n",
    "train_ids, dev_ids, train_labels, dev_labels = train_test_split(train_ids, train_labels, \n",
    "                                                                test_size=SRC_DEV_SPLIT/(1-SRC_TEST_SPLIT),\n",
    "                                                                random_state=0, stratify=train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the training set based on length\n",
    "Bucketing is done during NLP training to speed up the training time. To easily do this, the training set is sorted by its length. This is done so that there is finer control with the bucket compared to using tensorflow's bucketing functions\n",
    "\n",
    "Reference:\n",
    "*   Bucket Sequence by Length: https://www.tensorflow.org/api_docs/python/tf/contrib/data/bucket_by_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = list(range(len(train_ids)))\n",
    "train_indices.sort(key=lambda x: len(train_ids[x]))\n",
    "train_ids = [train_ids[idx] for idx in train_indices]\n",
    "train_labels = [train_labels[idx] for idx in train_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data to TFRecord\n",
    "\n",
    "To convert the ids and labels to TFRecords, the ```narau``` helpers are used.\n",
    "\n",
    "TFRecords are just protobufs with a defined format. Below are references that one can use to study them.\n",
    "\n",
    "*   Protocol Buffers: https://developers.google.com/protocol-buffers/\n",
    "*   Sequence Example: https://www.tensorflow.org/api_docs/python/tf/train/SequenceExample\n",
    "*   Tensorflow Examples Code: https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/core/example/example.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tfrecord(text, score, dst_path):\n",
    "    ex = nr.example.SequenceExample(\n",
    "        nr.example.FeatureLists({\n",
    "            'text': nr.example.Int64FeatureList(text),\n",
    "            'score': nr.example.FloatFeatureList(score),\n",
    "        })\n",
    "    )\n",
    "    nr.example.save_example(ex, dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to TFRecord\n",
    "\n",
    "Call the  ```convert_to_tfrecord``` to transform the different datasets on their own TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_tfrecord(train_ids, train_labels, DST_TRAIN_PATH)\n",
    "convert_to_tfrecord(test_ids, test_labels, DST_TEST_PATH)\n",
    "convert_to_tfrecord(dev_ids, dev_labels, DST_DEV_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
